{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f0f7742",
   "metadata": {},
   "source": [
    "## 7-2. 모델 크기 조절\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c69d318",
   "metadata": {},
   "source": [
    "### 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730b8a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encoding(data, dim=10000): # 아래 imdb.load_data의 num_words를 10000으로 설정할 예정이기 때문에 dim도 10000으로 맞춰줍니다.\n",
    "  results = np.zeros((len(data), dim))\n",
    "  for i, d in enumerate(data):\n",
    "    results[i, d] = 1.\n",
    "  return results\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "x_train = one_hot_encoding(train_data)\n",
    "x_test = one_hot_encoding(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6666f60",
   "metadata": {},
   "source": [
    "### ndarray의 element로 어떤 객체든지 올 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8622ba22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(25000,)\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(train_data))\n",
    "print(train_data.shape)\n",
    "print(type(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c33ef",
   "metadata": {},
   "source": [
    "### 모델 구성 및 컴파일\n",
    "- optimizer : rmsprop\n",
    "- loss : binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d644c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Dense)                (None, 128)               1280128   \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,296,769\n",
      "Trainable params: 1,296,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000, ), name='input'))\n",
    "model.add(layers.Dense(128, activation='relu', name='hidden'))\n",
    "model.add(layers.Dense(1, activation='sigmoid', name='output'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af95f727",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4fb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4242 - accuracy: 0.8006"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f41872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, loss, 'b--', label='train_loss')\n",
    "ax1.plot(epochs, val_loss, 'r--', label='val_loss')\n",
    "ax1.set_title('Train and Validation Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, accuracy, 'b--', label='train_accuracy')\n",
    "ax2.plot(epochs, val_accuracy, 'r--', label='val_accuracy')\n",
    "ax2.set_title('Train and Validation Accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc6ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_model = models.Sequential()\n",
    "b_model.add(layers.Dense(2048, activation='relu', input_shape=(10000, ), name='input3'))\n",
    "b_model.add(layers.Dense(2048, activation='relu', name='hidden3'))\n",
    "b_model.add(layers.Dense(1, activation='sigmoid', name='output3'))\n",
    "b_model.compile(optimizer='rmsprop',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "b_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5410ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_461/3922054738.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m b_model_history = b_model.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               validation_data=(x_test, y_test))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'b_model' is not defined"
     ]
    }
   ],
   "source": [
    "b_model_history = b_model.fit(x_train, y_train,\n",
    "                              epochs=30,\n",
    "                              batch_size=512, \n",
    "                              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_history_dict = b_model_history.history\n",
    "\n",
    "b_loss = b_history_dict['loss']\n",
    "b_val_loss = b_history_dict['val_loss']\n",
    "epochs = range(1, len(b_loss) + 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, b_loss, 'b-', label='train_loss(large)')\n",
    "ax1.plot(epochs, b_val_loss, 'r-', label='val_loss(large)')\n",
    "ax1.plot(epochs, loss, 'b--', label='train_loss')\n",
    "ax1.plot(epochs, val_loss, 'r--', label='val_loss')\n",
    "ax1.set_title('Train and Validation Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "b_accuracy = b_history_dict['accuracy']\n",
    "b_val_accuracy = b_history_dict['val_accuracy']\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, b_accuracy, 'b-', label='train_accuracy(large)')\n",
    "ax2.plot(epochs, b_val_accuracy, 'r-', label='val_accuracy(large)')\n",
    "ax2.plot(epochs, accuracy, 'b--', label='train_accuracy')\n",
    "ax2.plot(epochs, val_accuracy, 'r--', label='val_accuracy')\n",
    "ax2.set_title('Train and Validation Accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672e66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model = models.Sequential()\n",
    "s_model.add(layers.Dense(16, activation='relu', input_shape=(10000, ), name='input2'))\n",
    "s_model.add(layers.Dense(16, activation='relu', name='hidden2'))\n",
    "s_model.add(layers.Dense(1, activation='sigmoid', name='output2'))\n",
    "s_model.compile(optimizer='rmsprop',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "s_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model_history = s_model.fit(x_train, y_train,\n",
    "                              epochs=30,\n",
    "                              batch_size=512, \n",
    "                              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab90b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_history_dict = s_model_history.history\n",
    "\n",
    "s_loss = s_history_dict['loss']\n",
    "s_val_loss = s_history_dict['val_loss']\n",
    "epochs = range(1, len(s_loss) + 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, b_loss, 'b-', label='train_loss(large)')\n",
    "ax1.plot(epochs, b_val_loss, 'r-', label='val_loss(large)')\n",
    "ax1.plot(epochs, loss, 'b--', label='train_loss')\n",
    "ax1.plot(epochs, val_loss, 'r--', label='val_loss')\n",
    "ax1.plot(epochs, s_loss, 'b:', label='train_loss(small)')\n",
    "ax1.plot(epochs, s_val_loss, 'r:', label='val_loss(small)')\n",
    "ax1.set_title('Train and Validation Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "s_accuracy = s_history_dict['accuracy']\n",
    "s_val_accuracy = s_history_dict['val_accuracy']\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, b_accuracy, 'b-', label='train_accuracy(large)')\n",
    "ax2.plot(epochs, b_val_accuracy, 'r-', label='val_accuracy(large)')\n",
    "ax2.plot(epochs, accuracy, 'b--', label='train_accuracy')\n",
    "ax2.plot(epochs, val_accuracy, 'r--', label='val_accuracy')\n",
    "ax2.plot(epochs, s_accuracy, 'b:', label='train_accuracy(small)')\n",
    "ax2.plot(epochs, s_val_accuracy, 'r:', label='val_accuracy(small)')\n",
    "ax2.set_title('Train and Validation Accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f2a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [play ground]\n",
    "# 원하는 분은 Dense층을 늘리거나 줄이는 등 모델의 구조 역시 변경해보시기 바랍니다.\n",
    "\n",
    "your_model = models.Sequential()\n",
    "your_model.add(layers.Dense(4, activation='relu', input_shape=(10000, ), name='input2'))\n",
    "your_model.add(layers.Dense(4, activation='relu', name='hidden2'))\n",
    "your_model.add(layers.Dense(1, activation='sigmoid', name='output2'))\n",
    "your_model.compile(optimizer='rmsprop',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "your_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_model_history = your_model.fit(x_train, y_train, epochs=30, batch_size=512,  validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_history_dict = your_model_history.history\n",
    "\n",
    "your_loss = your_history_dict['loss']\n",
    "your_val_loss = your_history_dict['val_loss']\n",
    "epochs = range(1, len(your_loss) + 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, b_loss, 'b-', label='train_loss(large)')\n",
    "ax1.plot(epochs, b_val_loss, 'r-', label='val_loss(large)')\n",
    "ax1.plot(epochs, loss, 'b--', label='train_loss')\n",
    "ax1.plot(epochs, val_loss, 'r--', label='val_loss')\n",
    "ax1.plot(epochs, s_loss, 'b:', label='train_loss(small)')\n",
    "ax1.plot(epochs, s_val_loss, 'r:', label='val_loss(small)')\n",
    "ax1.plot(epochs, your_loss, 'g-', label='train_loss(your)')\n",
    "ax1.plot(epochs, your_val_loss, 'g--', label='val_loss(your)')\n",
    "ax1.set_title('Train and Validation Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "your_accuracy = your_history_dict['accuracy']\n",
    "your_val_accuracy = your_history_dict['val_accuracy']\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, b_accuracy, 'b-', label='train_accuracy(large)')\n",
    "ax2.plot(epochs, b_val_accuracy, 'r-', label='val_accuracy(large)')\n",
    "ax2.plot(epochs, accuracy, 'b--', label='train_accuracy')\n",
    "ax2.plot(epochs, val_accuracy, 'r--', label='val_accuracy')\n",
    "ax2.plot(epochs, s_accuracy, 'b:', label='train_accuracy(small)')\n",
    "ax2.plot(epochs, s_val_accuracy, 'r:', label='val_accuracy(small)')\n",
    "ax2.plot(epochs, your_accuracy, 'g-', label='train_accuracy(your)')\n",
    "ax2.plot(epochs, your_val_accuracy, 'g--', label='val_accuracy(your)')\n",
    "ax2.set_title('Train and Validation Accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5987c9",
   "metadata": {},
   "source": [
    "## 규제\n",
    "- session 메모리 문제가 있어서 여기서 다시 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd72c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input2 (Dense)               (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "hidden2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "output2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,305\n",
      "Trainable params: 160,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4457 - accuracy: 0.8295"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encoding(data, dim=10000): # 아래 imdb.load_data의 num_words를 10000으로 설정할 예정이기 때문에 dim도 10000으로 맞춰줍니다.\n",
    "  results = np.zeros((len(data), dim))\n",
    "  for i, d in enumerate(data):\n",
    "    results[i, d] = 1.\n",
    "  return results\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "x_train = one_hot_encoding(train_data)\n",
    "x_test = one_hot_encoding(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "s_model = models.Sequential()\n",
    "s_model.add(layers.Dense(16, activation='relu', input_shape=(10000, ), name='input2'))\n",
    "s_model.add(layers.Dense(16, activation='relu', name='hidden2'))\n",
    "s_model.add(layers.Dense(1, activation='sigmoid', name='output2'))\n",
    "s_model.compile(optimizer='rmsprop',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "s_model.summary()\n",
    "\n",
    "history = s_model.fit(x_train, y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_test, y_test))\n",
    "val_loss = history.history['val_loss']                    \n",
    "\n",
    "l1_model =  models.Sequential()\n",
    "l1_model.add(layers.Dense(16, \n",
    "                          kernel_regularizer='l1',\n",
    "                          activation='relu', \n",
    "                          input_shape=(10000, )))\n",
    "l1_model.add(layers.Dense(16, \n",
    "                          kernel_regularizer='l1',\n",
    "                          activation='relu'))\n",
    "l1_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "l1_model.compile(optimizer='rmsprop',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "l1_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_model_hist = l1_model.fit(x_train, y_train,\n",
    "                             epochs=30,\n",
    "                             batch_size=512,\n",
    "                             validation_data=(x_test, y_test))\n",
    "\n",
    "l1_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409367b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l1_model_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_546/687055121.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ml1_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml1_model_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'l1_model_hist' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "l1_val_loss = l1_model_hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, 31)\n",
    "plt.plot(epochs, val_loss, 'k--', label='Model')\n",
    "plt.plot(epochs, l1_val_loss, 'b--', label='L1-regularized')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model =  models.Sequential()\n",
    "l2_model.add(layers.Dense(16, \n",
    "                          kernel_regularizer='l2',\n",
    "                          activation='relu', \n",
    "                          input_shape=(10000, )))\n",
    "l2_model.add(layers.Dense(16, \n",
    "                          kernel_regularizer='l2',\n",
    "                          activation='relu'))\n",
    "l2_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "l2_model.compile(optimizer='rmsprop',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "l2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b641868",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model_hist = l2_model.fit(x_train, y_train,\n",
    "                             epochs=30,\n",
    "                             batch_size=512,\n",
    "                             validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_val_loss = l2_model_hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, 31)\n",
    "plt.plot(epochs, val_loss, 'k--', label='Model')\n",
    "plt.plot(epochs, l1_val_loss, 'b--', label='L1-regularized')\n",
    "plt.plot(epochs, l2_val_loss, 'r--', label='L2-regularized')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_l2_model =  models.Sequential()\n",
    "l1_l2_model.add(layers.Dense(16, \n",
    "                             kernel_regularizer='l1_l2',\n",
    "                             activation='relu', input_shape=(10000, )))\n",
    "l1_l2_model.add(layers.Dense(16, \n",
    "                             kernel_regularizer='l1_l2',\n",
    "                             activation='relu'))\n",
    "l1_l2_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "l1_l2_model.compile(optimizer='rmsprop',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "l1_l2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_l2_model_hist = l1_l2_model.fit(x_train, y_train,\n",
    "                                  epochs=30,\n",
    "                                  batch_size=512,\n",
    "                                  validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779cf157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [play ground]\n",
    "# L2 규제의 기본 값은 0.01입니다. 여러분이 원하는 크기로 조절해보세요. 혹은 다른 규제를 사용하셔도 됩니다.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "\n",
    "your_model =  models.Sequential()\n",
    "your_model.add(layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.1), activation='relu', input_shape=(10000, )))\n",
    "your_model.add(layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.1), activation='relu'))\n",
    "your_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "your_model.compile(optimizer='rmsprop',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "your_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_model_hist = your_model.fit(x_train, y_train,\n",
    "                                 epochs=30,\n",
    "                                 batch_size=512,\n",
    "                                 validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7080af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_val_loss_0_1 = your_model_hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, 31)\n",
    "plt.plot(epochs, val_loss, 'k--', label='Model')\n",
    "#plt.plot(epochs, l1_val_loss, 'b--', label='L1-regularized')\n",
    "plt.plot(epochs, l2_val_loss, 'r--', label='L2-regularize0.01')\n",
    "#plt.plot(epochs, l1_l2_val_loss, 'g--', label='L1_L2-regularized')\n",
    "plt.plot(epochs, your_val_loss_0_02, 'y--', label='L2-regularized 0.02')\n",
    "plt.plot(epochs, your_val_loss_0_1, 'b--', label='L2-regularized 0.1')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2babbe7",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47512bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a734162",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_50_history = model.fit(x_train, y_train,\n",
    "                            epochs=30,\n",
    "                            batch_size=512,\n",
    "                            validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_50_dict = drop_50_history.history\n",
    "\n",
    "drop_50_loss = drop_50_dict['loss']\n",
    "drop_50_val_loss = drop_50_dict['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, loss, 'b-', label='train_loss')\n",
    "ax1.plot(epochs, val_loss, 'r-', label='val_loss')\n",
    "ax1.plot(epochs, drop_20_loss, 'b--', label='train_loss (dropout 20%)')\n",
    "ax1.plot(epochs, drop_20_val_loss, 'r--', label='val_loss (dropout 20%)')\n",
    "ax1.plot(epochs, drop_50_loss, 'b:', label='train_loss (dropout 50%)')\n",
    "ax1.plot(epochs, drop_50_val_loss, 'r:', label='val_loss (dropout 50%)')\n",
    "ax1.set_title('Train and Validation Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "drop_50_accuracy = drop_50_dict['accuracy']\n",
    "drop_50_val_accuracy = drop_50_dict['val_accuracy']\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, accuracy, 'b-', label='train_accuracy')\n",
    "ax2.plot(epochs, val_accuracy, 'r-', label='val_accuracy')\n",
    "ax2.plot(epochs, drop_20_accuracy, 'b--', label='train_accuracy (dropout 20%)')\n",
    "ax2.plot(epochs, drop_20_val_accuracy, 'r--', label='val_accuracy (dropout 20%)')\n",
    "ax2.plot(epochs, drop_50_accuracy, 'b:', label='train_accuracy (dropout 50%)')\n",
    "ax2.plot(epochs, drop_50_val_accuracy, 'r:', label='val_accuracy (dropout 50%)')\n",
    "ax2.set_title('Train and Validation Accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
